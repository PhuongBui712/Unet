{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce7b6755",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8d093e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "897bb531c6956ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T04:13:08.923453Z",
     "start_time": "2024-06-26T04:13:07.505872Z"
    }
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from typing import Literal\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch import optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "\n",
    "from src.utils import get_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814ea667",
   "metadata": {},
   "source": [
    "## Define dataset, model and function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dad11d4",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d04e77a",
   "metadata": {},
   "source": [
    "**Mean** and **std** for image normalization. These values are suggested based on Imagenet after training on million of images.\n",
    "\n",
    "> All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]\n",
    "\n",
    "Access [here](https://pytorch.org/vision/0.8/models.html) for more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec060430",
   "metadata": {},
   "outputs": [],
   "source": [
    "MEAN = [0.485, 0.456, 0.406]\n",
    "STD = [0.229, 0.224, 0.225]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50aaa79c7c713a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BCSSDataset(Dataset):\n",
    "    SIZE=(224, 224)\n",
    "    _img_transformer = transforms.Compose([\n",
    "            transforms.Resize(SIZE),\n",
    "            transforms.PILToTensor(),\n",
    "            transforms.ConvertImageDtype(torch.float),\n",
    "            transforms.Normalize(mean=MEAN, std=STD),\n",
    "        ])\n",
    "    _mask_transformer = transforms.Compose([\n",
    "            transforms.Resize(SIZE),\n",
    "            transforms.PILToTensor(),\n",
    "        ])\n",
    "    \n",
    "    def __init__(self, image_path: str, mask_path: str):\n",
    "        image_path = os.path.abspath(image_path)\n",
    "        mask_path = os.path.abspath(mask_path)\n",
    "        \n",
    "        self.images = [os.path.join(image_path, filename) for filename in os.listdir(image_path)]\n",
    "        self.masks = [os.path.join(mask_path, filename) for filename in os.listdir(mask_path)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx: int):\n",
    "        image = Image.open(self.images[idx])\n",
    "        image = self._img_transformer(image)\n",
    "\n",
    "        mask = Image.open(self.masks[idx])\n",
    "        mask = self._mask_transformer(mask)\n",
    "        mask = torch.squeeze(mask, 0).long()\n",
    "\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89c57b2",
   "metadata": {},
   "source": [
    "### Unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8eb480da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels: int,\n",
    "                 out_channels: int,\n",
    "                 mid_channels: int = None,\n",
    "                 kernel_size: int = 3,\n",
    "                 stride: int = 1,\n",
    "                 padding: int = 0):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        mid_channels = mid_channels or out_channels\n",
    "        self.conv_ops = nn.Sequential(\n",
    "            # first \n",
    "            nn.Conv2d(in_channels=in_channels,\n",
    "                      out_channels=mid_channels,\n",
    "                      kernel_size=kernel_size,\n",
    "                      padding=padding,\n",
    "                      stride=stride),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(num_features=mid_channels),\n",
    "\n",
    "            # second\n",
    "            nn.Conv2d(in_channels=mid_channels,\n",
    "                      out_channels=out_channels,\n",
    "                      kernel_size=kernel_size,\n",
    "                      padding=padding,\n",
    "                      stride=stride),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(num_features=out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, X: Tensor):\n",
    "        res = self.conv_ops(X)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f486c205",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownSample(nn.Module):\n",
    "    def __init__(self,\n",
    "                 kernel_size: int = 2,\n",
    "                 stride: int = 1,\n",
    "                 padding: int = 0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(kernel_size, stride, padding)\n",
    "        \n",
    "    def forward(self, X: Tensor):\n",
    "        return self.pool(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a96aadde",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpSample(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels: int,\n",
    "                 out_channels: int,\n",
    "                 kernel_size: int = 2,\n",
    "                 stride: int = 1,\n",
    "                 padding: int = 0):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.up_conv = nn.ConvTranspose2d(in_channels=in_channels,\n",
    "                                          out_channels=out_channels,\n",
    "                                          kernel_size=kernel_size,\n",
    "                                          stride=stride,\n",
    "                                          padding=padding)\n",
    "        \n",
    "    def forward(self, X: Tensor):\n",
    "        return self.up_conv(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb1076b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CropAndConcat(nn.Module):\n",
    "    def forward(self, X: Tensor, contracting_X: Tensor):\n",
    "        contracting_X = transforms.functional.center_crop(\n",
    "            img=contracting_X,\n",
    "            output_size=(X.shape[2], X.shape[3])\n",
    "        )\n",
    "        X = torch.cat((X, contracting_X), dim=1)\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7df0a43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unet(nn.Module):\n",
    "    # TODO: Customize the conv blocks for easy-scalable\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        output_classes: int,\n",
    "        down_conv_kwargs: dict = None,\n",
    "        down_sample_kwargs: dict = None,\n",
    "        up_conv_kwargs: dict = None,\n",
    "        up_sample_kwargs: dict = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.down_conv = nn.ModuleList(\n",
    "            [\n",
    "                DoubleConv(in_channels=i, out_channels=o, **(down_conv_kwargs or {}))\n",
    "                for i, o in ((in_channels, 64), (64, 128), (128, 256), (256, 512))\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.down_sample = nn.ModuleList(\n",
    "            [DownSample(**(down_sample_kwargs or {})) for _ in range(4)]\n",
    "        )\n",
    "\n",
    "        self.up_conv = nn.ModuleList(\n",
    "            [\n",
    "                DoubleConv(in_channels=i, out_channels=o, **(up_conv_kwargs or {}))\n",
    "                for i, o in ((1024, 512), (512, 256), (256, 128), (128, 64))\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.up_sample = nn.ModuleList(\n",
    "            [\n",
    "                UpSample(in_channels=i, out_channels=o, **(up_sample_kwargs or {}))\n",
    "                for i, o in ((1024, 512), (512, 256), (256, 128), (128, 64))\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.crop_concat = nn.ModuleList([CropAndConcat() for _ in range(4)])\n",
    "\n",
    "        self.bottlekneck = DoubleConv(\n",
    "            in_channels=512, out_channels=1024, **(up_conv_kwargs or {})\n",
    "        )\n",
    "\n",
    "        self.output = nn.Conv2d(\n",
    "            in_channels=64, out_channels=output_classes, kernel_size=1\n",
    "        )\n",
    "\n",
    "    def forward(self, X: Tensor):\n",
    "        pass_through = []\n",
    "        for i in range(len(self.down_conv)):\n",
    "            X = self.down_conv[i](X)\n",
    "            pass_through = [X] + pass_through\n",
    "            X = self.down_sample[i](X)\n",
    "\n",
    "        X = self.bottlekneck(X)\n",
    "\n",
    "        for i in range(len(self.up_conv)):\n",
    "            X = self.up_sample[i](X)\n",
    "            X = self.crop_concat[i](X, pass_through[i])\n",
    "            X = self.up_conv[i](X)\n",
    "\n",
    "        X = self.output(X)\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915a404c",
   "metadata": {},
   "source": [
    "### Evaluation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcda9854",
   "metadata": {},
   "source": [
    "* Pixel Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c731397",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pixel_accuracy(logits: Tensor, masks: Tensor):\n",
    "    \"\"\"\n",
    "    Calculate the pixel accuracy of the predicted masks.\n",
    "\n",
    "    Args:\n",
    "        logits (Tensor): A tensor of shape (N, C, H, W) containing the logits for each class.\n",
    "        masks (Tensor): A tensor of shape (N, H, W) containing the ground truth masks.\n",
    "\n",
    "    Returns:\n",
    "        float: The pixel accuracy of the predicted masks.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        prob = F.softmax(logits, dim=1)\n",
    "        predicted_mask = torch.argmax(prob, dim=1)\n",
    "\n",
    "        correct_pred = (predicted_mask == masks)\n",
    "        accuracy = torch.sum(correct_pred).item() / correct_pred.numel()\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938b723c",
   "metadata": {},
   "source": [
    "* Mean Intersection over Union (Jaccard index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2a3498f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_iou(logits: Tensor, masks: Tensor, num_classes: int):\n",
    "    \"\"\"\n",
    "    Calculate the mean Intersection over Union (IoU) of the predictions.\n",
    "\n",
    "    Args:\n",
    "        logits (Tensor): A tensor of shape (N, C, H, W) containing the logits for each class.\n",
    "        masks (Tensor): A tensor of shape (N, H, W) containing the ground truth masks.\n",
    "        num_classes (int): The number of classes in the dataset.\n",
    "\n",
    "    Returns:\n",
    "        float: The mean IoU of the predicted masks.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        pred_masks = F.softmax(logits, dim=1)\n",
    "        pred_masks = torch.argmax(pred_masks, dim=1)\n",
    "\n",
    "        iou_per_class = []\n",
    "        for cls in range(num_classes):\n",
    "            pred_inds = (pred_masks == cls)\n",
    "            target_inds = (masks == cls)\n",
    "\n",
    "            union = (pred_inds | target_inds).sum().item()\n",
    "            if union == 0:\n",
    "                iou_per_class.append(np.nan)\n",
    "            else:\n",
    "                iou_per_class.append((pred_inds & union).sum().item() / union)\n",
    "\n",
    "        return np.nanmean(iou_per_class)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7f3ab0",
   "metadata": {},
   "source": [
    "### Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86ea427",
   "metadata": {},
   "source": [
    "As well as *Cross entropy or Binary cross entropy* depends on what kind of segmentation, **Dice loss** also a common function which is often used for segmentation problme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da3b94c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    DiceLoss class calculates the Dice coefficient loss, which is often used for \n",
    "    image segmentation tasks. This implementation supports both binary and \n",
    "    multiclass segmentation.\n",
    "\n",
    "    Args:\n",
    "        smooth (float): A smoothing constant to avoid division by zero errors. Default is 1e-10.\n",
    "\n",
    "    Methods:\n",
    "        forward(logits, masks):\n",
    "            Computes the Dice loss between the predicted logits and the ground truth masks.\n",
    "\n",
    "            Args:\n",
    "                logits (Tensor): A tensor of shape (N, C, H, W) containing the predicted logits for each class.\n",
    "                masks (Tensor): A tensor of shape (N, H, W) containing the ground truth masks.\n",
    "\n",
    "            Returns:\n",
    "                Tensor: The calculated Dice loss.\n",
    "    \"\"\"\n",
    "    def __init__(self, smooth: float = 1e-10):\n",
    "        super().__init__()\n",
    "        self.smooth = smooth\n",
    "\n",
    "    def forward(self, logits: Tensor, masks: Tensor):\n",
    "        # calculate probability for both logits and masks\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "\n",
    "        one_hot_masks = F.one_hot(masks, num_classes=probs.shape[1])\n",
    "        one_hot_masks = one_hot_masks.permute(0, 3, 1, 2).float()\n",
    "\n",
    "        # flatten for element-wise operations\n",
    "        probs = probs.view(probs.shape[0], probs.shape[1], -1)\n",
    "        one_hot_masks = one_hot_masks.view(one_hot_masks.shape[0], one_hot_masks.shape[1], -1)\n",
    "        # compute loss\n",
    "        intersection = torch.sum(probs * one_hot_masks, dim=2)\n",
    "        total = probs.sum(dim=2) + one_hot_masks.sum(dim=2)\n",
    "\n",
    "        dice_coef = 2 * intersection / total\n",
    "        avg_class_dice_coef = dice_coef.mean(dim=1)\n",
    "        loss = 1 - avg_class_dice_coef.mean() # mean for batch\n",
    "\n",
    "        return loss        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5fd471",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699ff319",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0581feb",
   "metadata": {},
   "source": [
    "* Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "375709ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_path = os.path.abspath('../data/bcss/train')\n",
    "train_mask_path = os.path.abspath('../data/bcss/train_mask')\n",
    "val_image_path = os.path.abspath('../data/bcss/val')\n",
    "val_mask_path = os.path.abspath('../data/bcss/val_mask')\n",
    "\n",
    "NUM_CLASSES = 22\n",
    "train_dataset = BCSSDataset(image_path=train_image_path, mask_path=train_mask_path)\n",
    "val_dataset = BCSSDataset(image_path=val_image_path, mask_path=val_mask_path)\n",
    "\n",
    "# Use Subset for testing\n",
    "train_dataset = Subset(train_dataset, list(range(50)))\n",
    "val_dataset = Subset(val_dataset, list(range(50)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7344707",
   "metadata": {},
   "source": [
    "* Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3841c4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = 16\n",
    "train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch, shuffle=True)\n",
    "val_dataloader = DataLoader(dataset=val_dataset, batch_size=batch, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4db1d2",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d615db",
   "metadata": {},
   "source": [
    "* Setup training utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dcf41b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else ('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aeb50aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Unet(\n",
    "    in_channels=3,\n",
    "    output_classes=22,\n",
    "    down_conv_kwargs={'kernel_size': 3, 'padding': 1},\n",
    "    down_sample_kwargs={'kernel_size': 2, 'stride': 2},\n",
    "    up_conv_kwargs={'kernel_size': 3, 'padding': 1},\n",
    "    up_sample_kwargs={'kernel_size': 2, 'stride': 2}\n",
    ")\n",
    "\n",
    "ce_loss = nn.CrossEntropyLoss().to(device)\n",
    "dice_loss = DiceLoss().to(device)\n",
    "\n",
    "max_lr = 1e-3\n",
    "weight_decay = 1e-4\n",
    "\n",
    "optimizer = optim.AdamW(params=model.parameters(), lr=1e-5, weight_decay=weight_decay)\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, max_lr=max_lr, epochs=epochs, steps_per_epoch=len(train_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1598f617",
   "metadata": {},
   "source": [
    "* Setup tracking with Wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "89176411",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbuitanphuong10c13\u001b[0m (\u001b[33mbtp712\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/btp712/Code/Unet/notebooks/wandb/run-20240707_160137-1wx108qt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/btp712/Computer-Vision/runs/1wx108qt' target=\"_blank\">[Unet] BCSS segmentation</a></strong> to <a href='https://wandb.ai/btp712/Computer-Vision' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/btp712/Computer-Vision' target=\"_blank\">https://wandb.ai/btp712/Computer-Vision</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/btp712/Computer-Vision/runs/1wx108qt' target=\"_blank\">https://wandb.ai/btp712/Computer-Vision/runs/1wx108qt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "wandb.login()\n",
    "\n",
    "wandb.init(\n",
    "    name='[Unet] BCSS segmentation',\n",
    "    config={\n",
    "        'epoch': epochs,\n",
    "        'batch_size': batch\n",
    "    },\n",
    ")\n",
    "\n",
    "STEP_PER_LOG = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a91531",
   "metadata": {},
   "source": [
    "* Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a92c076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: save checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb3bc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tracking variables\n",
    "train_loss, val_loss, train_acc, val_acc, train_iou, val_iou, lrs = [], [], [], [], [], [], []\n",
    "\n",
    "model.to(device)\n",
    "for epoch in range(epochs):\n",
    "    running_loss, iou_score, accuracy = 0, 0, 0\n",
    "    batch_count, num_log = 0, 1\n",
    "    last_train_data = None\n",
    "\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    train_loop = tqdm(train_dataloader, desc=f'Training Epoch {epoch+1}/{epochs}', leave=True)\n",
    "    for i, data in enumerate(train_loop):\n",
    "        X, y = (_.to(device) for _ in data)\n",
    "\n",
    "        # Forward\n",
    "        y_pred = model(X)\n",
    "\n",
    "        # compute loss\n",
    "        loss = dice_loss(y_pred, y) + ce_loss(y_pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # update metrics\n",
    "        running_loss += loss.item()\n",
    "        iou_score += mean_iou(y_pred, y, num_classes=NUM_CLASSES)\n",
    "        accuracy += pixel_accuracy(y_pred, y)\n",
    "\n",
    "        # update progress bar\n",
    "        logging_dict = {\n",
    "            'loss': running_loss / (i + 1),\n",
    "            'mean IoU': iou_score / (i + 1),\n",
    "            'accuracy': accuracy / (i + 1)\n",
    "        }\n",
    "        train_loop.set_postfix(logging_dict)\n",
    "\n",
    "        # step learning rate scheduler\n",
    "        lrs.append(get_lr(optimizer))\n",
    "        scheduler.step()\n",
    "\n",
    "        # update wandb\n",
    "        batch_count += 1\n",
    "        if batch_count // STEP_PER_LOG == num_log or i == len(train_dataloader) - 1:\n",
    "            logging_dict['epoch'] = batch_count / len(train_dataloader)\n",
    "            wandb.log({f'train/{k}': v for k, v in logging_dict.items()}, step=batch_count)\n",
    "            \n",
    "            num_log += 1\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_running_loss, val_iou_score, val_accuracy = 0, 0, 0\n",
    "    val_loop = tqdm(val_dataloader, desc='Validation', leave=True)\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(val_loop):\n",
    "            X, y = (_.to(device) for _ in data)\n",
    "\n",
    "            # Forward\n",
    "            y_pred = model(X)\n",
    "\n",
    "            # compute loss\n",
    "            loss = dice_loss(y_pred, y) + ce_loss(y_pred, y)\n",
    "\n",
    "            # update metrics\n",
    "            val_running_loss += loss.item()\n",
    "            val_iou_score += mean_iou(y_pred, y, num_classes=NUM_CLASSES)\n",
    "            val_accuracy += pixel_accuracy(y_pred, y)\n",
    "\n",
    "            # update progress bar\n",
    "            logging_dict = {\n",
    "                'loss': val_running_loss / (i + 1),\n",
    "                'mean IoU': val_iou_score / (i + 1),\n",
    "                'accuracy': val_accuracy / (i + 1)\n",
    "            }\n",
    "            val_loop.set_postfix(logging_dict)\n",
    "\n",
    "    # Log the evaluation data together with train data\n",
    "    wandb.log({\n",
    "        'train/epoch': epoch + 1,\n",
    "        'eval/loss': val_running_loss / len(val_dataloader),\n",
    "        'eval/mean IoU': val_iou_score / len(val_dataloader),\n",
    "        'eval/accuracy': val_accuracy / len(val_dataloader)\n",
    "    })\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
